# 记录VLM/VLA的推理加速相关

# qwen3 vs qwen2.5
1. 对比qwen3-vl-2b 和 qwen2.5-vl-3b，由于参数量的减少，qwen3-vl-2b要快1.5X，在fp16精度和h800的环境下
* qwen2.5-vl-3b: 480个vit token的情况下，vit耗时11.5ms,0.02ms/token， prefill 阶段41.02ms(550个token),0.07ms/toekn, decoding阶段3.79ms/token
* qwen3-vl-2b: 480个vit token的情况下，vit耗时7.6ms,0.015ms/token， prefill 阶段27.11ms(550个token),0.04ms/toekn, decoding阶段2.77ms/token

2. 量化相关
* FP8量化
** vit阶段，FP8的精度相对FP16的精度，时延7.61ms->6.99ms(提速1.09X），分析发现只有gemm跑在FP8,attention使用flash_attantion库，跑在FP16.
** prefill阶段，相比FP16,FP8的时延没有变化，原因是prefill阶段的瓶颈在于计算
** decode阶段，FP8的时延2.77ms/token->2.3ms/token(提速1.2X)

* INT4量化(W4A16，awq算法)
** prefill阶段，int4量化的时延出现恶化(相比FP16恶化1.11x)，原因是prefill阶段的瓶颈在于计算
** decode阶段，int4量化的时延2.77ms/token->2ms/token(提速1.39X),原因是decode阶段属于访存瓶颈，访存的收益大于反量化的恶化。

* INT8量化(W8A8，smoothquant)
** 

* 显存变化，FP16+FP16的峰值显存是7.5G,FP8+FP8的峰值显存是5.8G,FP8+INT4的峰值显存是5.5G


3. 投机采样


4. 视觉token裁剪


5. 蒸馏 and MOE模型 and 剪枝

6. 词表增删
* 增加，在自动驾驶行业，需要增加历史轨迹/状态的词表
* 删除，删除非自动驾驶行业且不常用的词表，如专用名称
* 替换，是大模型压缩和高效微调的一个重要技术，主要是找到与当前场景无关的词表，替换为当前场景需要的词表，方法包括频率统计、相关性计算和先验性查找。
* 模型上的修改，token之后，修改embedding的weight大小，修改输出head的映射

7. kv-cache优化
* pagedAttention,思想是将kv缓存分页管理，类似虚拟内存，消除显存碎片，提升利用率达到20倍
* Multi-Query/Grouped-Query，思想是减少K、V头的数量，直接减少KV缓存的大小
* kv缓存量化，将FP16的kv缓存转换为INT8/INT4
* 滑动窗口注意力，只保存最近N个token的KV缓存，缓存大小固定
* 丢弃不重要Token的KV，基于注意力分数丢弃历史Token
